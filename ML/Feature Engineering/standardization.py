# -*- coding: utf-8 -*-
"""Standardization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M8j9BC8E93EeYtOfQF42c9OCVt-HWLlY
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv(r'D:\CODING_CODES\AIML\ML\Feature Engineering\Data\Social_Network_Ads.csv')
print(df.head())

df = df[['Age','EstimatedSalary','Purchased']]

print(df.head())

print(df.info())

X = df.drop('Purchased',axis=1)
y = df['Purchased']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state=4)

# Before scaling
plt.scatter(df['Age'],df['EstimatedSalary'])
plt.show()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(X_train) # only fit training data

# transform normal data into scaled data
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
# .transform returns an ndarray so its better to convert into dataframe

print(type(X_train_scaled))

X_train_scaledDF = pd.DataFrame(X_train_scaled,columns=X_train.columns)
X_test_scaledDF = pd.DataFrame(X_test_scaled,columns=X_test.columns)
print(type(X_train_scaledDF))

print(X_train.head())
print(X_train_scaledDF.head())

plt.scatter(X_train['Age'],X_train['EstimatedSalary'])
plt.show()

plt.scatter(X_train_scaledDF['Age'],X_train_scaledDF['EstimatedSalary'])
plt.show()

print(X_train.describe())
print(X_train_scaledDF.describe())

print(X_train.mean())
print(X_train_scaledDF.mean())

print(X_train.std())
print(X_train_scaledDF.std())

# the mean and standard deviation of scaled data will be nearly 0 and 1 respectively
# the outliers are not affected by scaling. they remain at the same distance from non-outliers
# When to use standardization? -> K means, K nearest neighbours, PCA, artificial neural networks, gradient descent


# formula used here: Xi' = Xi- Xmean/Xstd