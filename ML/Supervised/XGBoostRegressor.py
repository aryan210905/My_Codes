# -*- coding: utf-8 -*-
"""XGBRegressor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZaRO6pbLFR3s5X1l_2cYMalLUMgNtqED
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import r2_score
import xgboost

data = pd.read_csv(r'D:\CODING_CODES\AIML\ML\Supervised\Data\used_cars_data.csv')

def scaling(X_train,X_test):
  # Scaling
  scaler = StandardScaler()
  X_train_scaled = scaler.fit_transform(X_train)
  X_test_scaled = scaler.transform(X_test)
  return X_train_scaled,X_test_scaled,scaler

def encoder(X_train,X_test):
  '''
  Encoding Categorical data:
  Fuel_Type, Transmission = OHE
  Owner_Type = Ordinal Encoder
  '''
  transformer = ColumnTransformer(transformers=[
    ('tnf1',OneHotEncoder(sparse_output=False,dtype=np.int32,drop='first'),['Location','Fuel_Type','Transmission']),
    ('tnf2',OrdinalEncoder(categories=[['First','Second','Third','Fourth & Above']]),['Owner_Type'])
  ],remainder='passthrough')

  # Encoding
  X_train_new = transformer.fit_transform(X_train)
  X_test_new = transformer.transform(X_test)

  return X_train_new,X_test_new,transformer

def preprocessing(data):
  data = data.drop(['S.No.','Name'],axis=1)

  # Filtering columns by removing units
  data['Engine_Filtered'] = data['Engine'].str.extract(r'(\d+)')
  data['Power_Filtered'] = data['Power'].str.extract(r'(\d*\.?\d*)')
  data['Mileage'] = data['Mileage'].str.replace(r'\s*(km/kg|kmpl)','',regex=True)
  data['New_Price'] = data['New_Price'].str.replace(r'\s*(Lakh)','',regex=True)

  # typecasting each column
  data['Engine_Filtered'] = pd.to_numeric(data['Engine_Filtered'], errors='coerce')
  data['Power_Filtered'] = pd.to_numeric(data['Power_Filtered'], errors='coerce')
  data['Mileage'] = pd.to_numeric(data['Mileage'],errors='coerce')
  data['New_Price'] = pd.to_numeric(data['New_Price'],errors='coerce')

  # handling null values
  data = data.dropna(subset=['New_Price','Price'],how='all')
  data['Mileage'] = data['Mileage'].fillna(data['Mileage'].mean())
  data['Seats'] = data['Seats'].fillna(data['Seats'].mean())
  data['Engine_Filtered'] = data['Engine_Filtered'].fillna(data['Engine_Filtered'].mean())
  data['Power_Filtered'] = data['Power_Filtered'].fillna(data['Power_Filtered'].mean())

  # merging Price and New_Price columns
  conditions = [
    ((data['New_Price'].notna())),
    ((data['New_Price'].isna()) & (data['Price'].notna()))
  ]
  choices = [
      data['New_Price'],
      data['Price']
  ]
  data['Final_Price'] = np.select(conditions,choices,default=np.nan)

  # removing extra columns
  data = data.drop(['Engine','Power'],axis=1)
  data = data.drop(['New_Price','Price'],axis=1)

  # Train Test Split
  X = data.drop('Final_Price',axis=1)
  y = data['Final_Price']
  X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)

  X_train,X_test,transformer = encoder(X_train,X_test)
  X_train,X_test,scaler = scaling(X_train,X_test)
  return X_train,X_test,y_train,y_test,transformer,scaler

X_train,X_test,y_train,y_test,transformer,scaler = preprocessing(data)

# non tuned XG Boost Regressor

xgb = xgboost.XGBRegressor()

xgb.fit(X_train,y_train)

print(r2_score(y_test,xgb.predict(X_test)))

# tuned xgb regressor

estimators = {
    'objective': ['reg:squarederror','reg:squaredlogerror','reg:absoluteerror'],
    'n_estimators':[50,100,200,250,300,500],  # no of trees
    'max_depth': range(1,11),   # max depth of each tree
    'learning_rate': [0.01,0.02,0.05,0.1,0.25,0.5,0.75,1],   # weightage of each tree
    'min_child_weight': range(1,11),   # minimum leaf node size
    'gamma' : [0.01,0.05,0.1,0.5,1,2,3,4],  # minimum loss reduction needed for further split of a leaf node
    'subsample' : [0.1,0.25,0.5,0.75],  # proportion of rows to give for randomness
    'colsample_bytree': [0.1,0.25,0.5,0.75],  # proportion of cols to give for randomness
    'lambda': [0.01,0.05,0.1,0.5,1,2,3,4],   # L2 regularization on weights
    'alpha': [0.01,0.05,0.1,0.5,1,2,3,4],    # L1 regularization on weights
    'tree_method':['auto','hist','gpu_hist'],  # hist -> speed, gpu_hist -> gpu
    'eval_metric':['rmse','mae','rmsle']
}

tuner = RandomizedSearchCV(xgboost.XGBRegressor(),param_distributions=estimators,cv=5,scoring='r2',n_jobs=-1,verbose=1)

tuner.fit(X_train,y_train)

print(tuner.best_estimator_)
print(tuner.best_score_)
print(tuner.best_params_)

