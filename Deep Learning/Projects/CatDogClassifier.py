# -*- coding: utf-8 -*-
"""CatDogClassifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HLNR-JX7U6hOcr9j4Bi5sP6AAiuUQecM
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import cv2
from sklearn.preprocessing import StandardScaler
import tensorflow
from tensorflow import keras
from keras import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout

import kagglehub

# Download latest version
path = kagglehub.dataset_download("salader/dogs-vs-cats")

print("Path to dataset files:", path)

# generators: creates batches of data
# used to process large amount of data

train_data = keras.utils.image_dataset_from_directory(
    directory='/kaggle/input/dogs-vs-cats/dogs_vs_cats/train',
    labels='inferred',
    label_mode = 'int',
    batch_size = 32,
    image_size = (256,256)
)

test_data = keras.utils.image_dataset_from_directory(
    directory='/kaggle/input/dogs-vs-cats/dogs_vs_cats/test',
    labels='inferred',
    label_mode = 'int',
    batch_size = 32,
    image_size = (256,256)
)

# normalize
def process(img,label):
  img = tensorflow.cast(img/255,tensorflow.float32)
  return img,label

train_ds = train_data.map(process)
test_ds = test_data.map(process)

# create CNN model

model = Sequential()

model.add(Conv2D(32,kernel_size=(3,3),padding='valid',activation='relu',input_shape=(256,256,3)))
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Conv2D(64,kernel_size=(3,3),padding='valid',activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Conv2D(128,kernel_size=(3,3),padding='valid',activation='relu'))
model.add(MaxPooling2D(pool_size=(2,2),strides=2,padding='valid'))

model.add(Flatten())

model.add(Dense(128,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(Dense(64,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(Dense(32,activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.1))

model.add(Dense(1,activation='sigmoid'))

model.summary()

model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='Adam')

history = model.fit(train_ds, epochs=10, validation_data = test_ds)

